{
    "input_dim": 26,
    "layers": [
        {
            "activation": "ReLU",
            "units": 256,
            "name": "layer1"
        },
        {
            "activation": "ReLU",
            "units": 128,
            "name": "layer2"
        },
        {
            "activation": "Linear",
            "units": 64,
            "name": "layer3"
        },
        {
            "activation": "Linear",
            "units": 2,
            "name": "layer3"
        },
        {
            "activation": "Softmax",
            "units": 2,
            "name": "output_layer"
        }
    ],
    "loss": "BinaryCrossEntropy",
    "optimizer": "Adam",
    "lr": 0.001,
    "metrics": {"accuracy": "categorical_accuracy"}
}